# Section for configuring the postgres database that will be used by qualitytrace
postgresql:
  # For now, this is required to be enabled, otherwise qualitytrace will not
  # be able to function properly.
  enabled: true
  architecture: standalone
  image:
    registry: ghcr.io
    repository: kube-tarian/helmrepo-supporting-tools/postgresql
    tag: 14.7.0-debian-11-r29

  # credentials for accessing the database
  auth:
    database: "qualitytrace"
    username: "qualitytrace"
    password: not-secure-database-password
    existingSecret: ""
  
  ## @section Metrics Parameters
  metrics:
    ## @param metrics.enabled Start a prometheus exporter
    enabled: false
    ## Prometheus Operator ServiceMonitor configuration
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      enabled: false

# Provisioning allows to define initial settings to be loaded into the database on first run.
# These will only be applied when running qualitytrace against an empty database. If qualitytrace has already
# been configured, the provisioning settings are ignored.
provisioning:
  # Datastore is where your application stores its traces. You can define several different datastores with
  # different names, however, only one is used by qualitytrace.
  type: DataStore
  spec:
    id: current
    name: Signoz
    type: signoz

  # type: DataStore
  # spec:
  #   name: Jaeger
     # Indicates that this datastore is a jaeger instance
  #   type: jaeger
     # Configures how qualitytrace connects to jaeger.
  #   jaeger:
  #     endpoint: jaeger-query:16685
  #     tls:
  #       insecure: true


# This section configures the strategy for pooling traces from the trace backend
poolingConfig:
  # How long qualitytrace will wait for a complete trace before timing out
  # If you have long-running operations that will generate part of your trace, you have
  # to change this attribute to be greater than the execution time of your operation.
  maxWaitTimeForTrace: 30s

  # How long qualitytrace will wait to retry fetching the trace. If you define it as 5s it means that
  # qualitytrace will retrieve the operation trace every 5 seconds and check if it's complete. It will
  # do that until the operation times out.
  retryDelay: 5s

# Section for anonymous analytics. If it's enabled, qualitytrace will collect anonymous analytics data
# to help us improving our project. If you don't want to send analytics data, set enabled as false.
analytics:
  enabled: true

# Section to configure how telemetry works in qualitytrace.
telemetry:
  # exporters are opentelemetry exporters. It configures how qualitytrace generates and send telemetry to
  # a datastore.
  exporters:
    # This is an exporter called collector, but it could be named anything else.
    collector:
      # configures the service.name attribute in all trace spans generated by qualitytrace
      serviceName: qualitytrace
      # indicates the percentage of traces that would be sent to the datastore. 100 = 100%
      sampling: 100
      # configures the exporter
      exporter:
        # qualitytrace sends data using the opentelemetry collector. For now there is no other
        # alternative. But the collector is flexible enough to send traces to any other tracing backend
        # you might need.
        type: collector
        collector:
          # endpoint to send traces to the collector
          endpoint: qualitytrace-otel-collector:4317

# Configures the server
server:
  # Indicates the port that qualitytrace will run ons
  httpPort: 11633
  otlpGrpcPort: 4317
  otlpHttpPort: 4318

  # Indicates which telemetry components will be used by qualitytrace
  telemetry:
    # The exporter that qualitytrace will use to send telemetry
    exporter: collector
    # The exporter that qualitytrace will send the triggering transaction span. This is optional. If you
    # want to have the trigger transaction span in your trace, you have to configure this field to
    # send the span to the same place your application sends its telemetry.
    # applicationExporter: collector

replicaCount: 1

# You don't have to change anything bellow this line.

image:
  repository: ghcr.io/intelops/qualitytrace
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: latest

env:
  qualitytraceDev: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

analyzer:
  enabled: false

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  annotations: {}

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  requests:
    cpu: 250m
    memory: 512Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}


otelCollector:
  name: "otel-collector"
  image:
    repository: ghcr.io/kube-tarian/helmrepo-supporting-tools/opentelemetry-collector-contrib
    tag: 0.79.0
    pullPolicy: Always

  # -- Image Registry Secret Names for OtelCollector
  # If set, this has higher precedence than the root level or global value of imagePullSecrets.
  imagePullSecrets: []

  configMap:
    # -- Specifies whether a configMap should be created (true by default)
    create: true

  # OtelCollector service
  service:
    # -- Annotations to use by service associated to OtelCollector
    annotations: {}
    # -- Service Type: LoadBalancer (allows external access) or NodePort (more secure, no extra cost)
    type: ClusterIP

  # -- OtelCollector Deployment annotation.
  annotations: {}
  # -- OtelCollector pod(s) annotation.
  podAnnotations: {}
  # -- OtelCollector pod(s) labels.
  podLabels: {}

  # -- Additional environments to set for OtelCollector
  additionalEnvs: {}
    # env_key: env_value

  # Configuration for ports
  ports:
    grpc:
      # -- Whether to enable service port for OTLP gRPC
      enabled: true
      # -- Container port for OTLP gRPC
      containerPort: 4317
      # -- Service port for OTLP gRPC
      servicePort: 4317
      # -- Node port for OTLP gRPC
      nodePort: ""
      # -- Protocol to use for OTLP gRPC
      protocol: TCP
    http:
      # -- Whether to enable service port for OTLP HTTP
      enabled: true
      # -- Container port for OTLP HTTP
      containerPort: 4318
      # -- Service port for OTLP HTTP
      servicePort: 4318
      # -- Node port for OTLP HTTP
      nodePort: ""
      # -- Protocol to use for OTLP HTTP
      protocol: TCP

  ingressRoute:
    enabled: false
    annotations: {}
    host: "otelcollector.domain.com"
    service:
      port: 4317
    tls: []
    #  - secretName: chart-example-tls

  ingress:
    # -- Enable ingress for OtelCollector
    enabled: false
    # -- Ingress Class Name to be used to identify ingress controllers
    className: ""
    # -- Annotations to OtelCollector Ingress
    annotations: {}
      # cert-manager.io/cluster-issuer: letsencrypt-prod
      # nginx.ingress.kubernetes.io/ssl-redirect: "true"
      # nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # -- OtelCollector Ingress Host names with their path details
    hosts:
      - host: otelcollector.domain.com
        paths:
          - path: /
            pathType: ImplementationSpecific
            port: 4317
    # -- OtelCollector Ingress TLS
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - otelcollector.domain.com

  # -- Configure resource requests and limits. Update according to your own use
  # case as these values might not be suitable for your workload.
  # Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  #
  # @default -- See `values.yaml` for defaults
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    # limits:
    #   cpu: "1"
    #   memory: 2Gi

  # -- OtelCollector priority class name
  priorityClassName: ""
  # -- Node selector for settings for OtelCollector pod
  nodeSelector: {}
  # -- Toleration labels for OtelCollector pod assignment
  tolerations: []
  # -- Affinity settings for OtelCollector pod
  affinity: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 11
    targetCPUUtilizationPercentage: 50
    #targetMemoryUtilizationPercentage: 50

  # -- Configurations for OtelCollector
  # @default -- See `values.yaml` for defaults
  config:
    receivers:
      otlp:
        protocols:
          grpc:
          http:

    processors:
      batch:
        timeout: 100ms

      # Data sources:
      probabilistic_sampler:
        hash_seed: 22
        sampling_percentage: 100

    exporters:
      # Output logger, used to check OTel Collector sanity
      logging:
        loglevel: debug

      # OTLP for qualitytrace
      otlp/qualitytrace:
        endpoint: qualitytrace:4317
        tls:
          insecure: true
      # OTLP for Signoz
      otlp/signoz:
        endpoint: signoz-otel-collector:4317
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [probabilistic_sampler, batch]
          exporters: [otlp/signoz, otlp/qualitytrace, logging]


externalPostgresql:
  host: postgresql
  database: ""
  username: ""
  password: ""
  postgresqlPassword: ""
  existingSecret: {}
    # name:
    # passwordKey: